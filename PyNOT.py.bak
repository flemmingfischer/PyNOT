# -*- coding: UTF-8 -*-
"""
  PyNOT - longslit reduction

  The module handles longslit reduction for the ALFOSC instrument
  at the Nordic Optical Telescope (NOT).
  Written by Jens-Kristian Krogager, Mar 2017

  depends on matplotlib, numpy, scipy, pyfits, argparse, astroscrappy

  .. functions:
   - combine_bias_frames
   - combine_flat_frames
   - normalize_spectral_flat
   - fit_background
   - science_reduction_2d
"""
__version__ = '0.5.0'
__author__ = 'Jens-Kristian Krogager'
__email__ = "krogager@iap.fr"
__credits__ = ["Jens-Kristian Krogager"]

import numpy as np
import matplotlib.pyplot as plt
import pyfits as pf
from scipy.ndimage import gaussian_filter1d, median_filter
from numpy.polynomial import Chebyshev
import os
from os.path import exists, basename, isfile
from matplotlib import ticker
from argparse import ArgumentParser
import warnings

try:
    from astroscrappy import detect_cosmics
    astroscrappy_installed = True
except:
    astroscrappy_installed = False

from extraction import extract_and_calibrate
import alfosc_data as alfosc


def my_formatter(x, p, scale_pow):
    """Format tick marks to exponential notation"""
    return "%.0f" % (x / (10 ** scale_pow))


def mad(img):
    """Calculate Median Absolute Deviation from the median
    This is a robust variance estimator.
    For a Gaussian distribution:
        sigma â‰ˆ 1.4826 * MAD
    """
    return np.median(np.abs(img - np.median(img)))


def combine_bias_frames(bias_frames, output='', kappa=15, clobber=False, verbose=False):
    """Combine individual bias frames to create a 'master bias' frame.
    The combination is performed using robust sigma-clipping and
    median combination. Bad pixels are subsequently replaced by the
    median value of the final combined image.

    Parameters
    ==========

    bias_frames : list of strings, or other iterable
        List containing file names for the individual bias frames

    output : string [default='']
        Output file name for the final combined image.

    kappa : integer [default=15]
        Number of sigmas above which to reject pixels.

    blobber : boolean [default=False]
        Overwrite existing output file if True.

    Returns
    =======

    master_bias : np.array (N, M)
        Final median combined bias frame, same shape as input images.
    """
    bias = list()
    for frame in bias_frames:
        bias.append(pf.getdata(frame))

    mask = np.zeros_like(bias[0], dtype=int)
    median_img0 = np.median(bias, 0)
    sig = mad(median_img0)*1.4826
    median = np.median(median_img0)
    masked_bias = list()
    for img in bias:
        this_mask = np.abs(img - median) > kappa*sig
        masked_bias.append(np.ma.masked_where(this_mask, img))
        mask += 1*this_mask

    master_bias = np.median(masked_bias, 0)
    Ncomb = len(bias) - mask

    master_bias[Ncomb == 0] = np.median(master_bias[Ncomb != 0])

    if output:
        hdr = pf.getheader(bias_frames[0], 0)
        hdr1 = pf.getheader(bias_frames[0], 1)
        for key in hdr1.keys():
            hdr[key] = hdr1[key]
        hdr['NCOMBINE'] = len(bias_frames)
        hdr.add_comment('Median combined Master Bias')
        hdr.add_comment('PyNOT version %r' % __version__)

        if output[-5:] == '.fits':
            pass
        else:
            output += '.fits'
    else:
        output = 'MASTER_BIAS.fits'

    pf.writeto(output, master_bias, header=hdr, clobber=clobber)
    if verbose:
        print " Saved output: "+output

    return master_bias


def combine_flat_frames(raw_frames, mbias='', output='', match_slit='',
                        kappa=5, verbose=False, clobber=False):
    """Combine individual spectral flat frames to create a 'master flat' frame.
    The individual frames are normalized to the mode of the 1D collapsed spectral
    shape. Individual frames are clipped using a kappa-sigma-clipping on the mode
    values to discard outliers.
    The input frames are by default matched to a given slit-width, though this can
    be turned off. The variations from one slit to another is very small.
    The normalized 2D frames are then median combined and the final image is
    multiplied by the median normalization to restore the ADU values of the image.

    Parameters
    ==========

    raw_frames : list of strings, or other iterable
        List containing file names for the individual flat frames

    mbias : string [default='']
        Master bias file name to subtract bias from individual frames.
        If nothing is given, no bias level correction is performed.

    output : string [default='']
        Output file name for the final combined image.

    match_slit : string [default='Slit_1.0']
        Slit name to match, if a flat frame is not taken with the
        given slit, it is not included in the combination.
        This is following NOT/ALFOSC naming from 'ALAPRTNM' in the header.

    kappa : integer [default=15]
        Number of sigmas above which to reject pixels.

    verbose : boolean [default=False]
        If True, print status messages.

    clobber : boolean [default=False]
        Overwrite existing output file if True.

    Returns
    =======

    flat_combine : np.array (N, M)
        Final median combined flat frame, same shape as input images.
    """
    if mbias and exists(mbias):
        bias = pf.getdata(mbias)
    else:
        if verbose:
            print "  WARNING - No master bias frame provided!"
        bias = 0.

    flats = list()
    flat_peaks = list()
    if verbose:
        print""
    for fname in raw_frames:
        hdr = pf.getheader(fname)
        if match_slit != '' and match_slit in alfosc.slits:
            if hdr['ALAPRTNM'] == match_slit:
                flat = pf.getdata(fname)
                flat = flat - bias
                peak_val = np.max(np.mean(flat, 1))
                flats.append(flat/peak_val)
                flat_peaks.append(peak_val)
                if verbose:
                    print " Appended file: %s   mode=%.1f" % (fname, peak_val)

        elif match_slit == '':
            flat = pf.getdata(fname)
            flat = flat - bias
            peak_val = np.max(np.mean(flat, 1))
            flats.append(flat/peak_val)
            flat_peaks.append(peak_val)
            if verbose:
                print " Appended file: %s   mode=%.1f" % (fname, peak_val)

        else:
            print "Invalid Slit Name given:  %s" % match_slit
            return None

    # Perform robust clipping using median absolute deviation
    # Assuming a Gaussian distribution:
    sigma = mad(flat_peaks)*1.4826
    median = np.median(flat_peaks)
    for i, peak_val in enumerate(flat_peaks):
        if np.abs(peak_val - median) > kappa*sigma:
            flats.pop(i)

    flat_combine = np.median(flats, 0) * median
    if verbose:
        print " Combined %i files" % len(flats)

    if output:
        hdr = pf.getheader(raw_frames[0], 0)
        hdr1 = pf.getheader(raw_frames[0], 1)
        for key in hdr1.keys():
            hdr[key] = hdr1[key]
        hdr['NCOMBINE'] = len(flats)
        hdr.add_comment('Median combined Master Spectral Flat')
        hdr.add_comment('PyNOT version %r' % __version__)

        if output[-5:] == '.fits':
            pass
        else:
            output += '.fits'
    else:
        grism = alfosc.grism_translate[hdr['ALGRNM']]
        output = 'FLAT_COMBINED_%s.fits' % grism

    pf.writeto(output, flat_combine, header=hdr, clobber=clobber)
    if verbose:
        print " Saved output: "+output

    return output


def normalize_spectral_flat(fname, output='', axis=1, x1=0, x2=2050, order=24, sigma=5,
                            plot=True, show=True, ext=1, clobber=False, verbose=False):
    """
    Normalize spectral flat field for long-slit observations. Parameters are optimized
    for NOT/ALFOSC spectra with horizontal slits, i.e., vertical spectra [axis=1],
    and grism #4.
    In order to keep the edges from diverging greatly, the code uses a relatively
    low polynomial order to fit the edges while using smoothing to recover the central
    part of the spectral shape.
    The two parts are then stiched together to create the final 1D profile.

    Parameters
    ==========

    fname : string
        Input FITS file with raw lamp flat data

    output : string [default='']
        Filename of normalized flat frame, if not given the output is not saved to file

    axis : integer [default=1]
        Dispersion axis, 0: horizontal spectra, 1: vertical spectra

    x1 : integer [default=0]
        Mask pixels below this number in the fit to the spectral shape

    x2 : integer [default=1028]
        Mask pixels above this number in the fit to the spectral shape

    order : integer [default=24]
        Order for Chebyshev polynomial to fit to the spectral shape.

    plot : boolean [default=True]
        Plot the 1d and 2d data for inspection?

    show : boolean [default=True]
        Show the figures directly or just save to file? If False, the figures will only be saved
        as pdf files.

    ext : integer [default=1]
        File extension to open, default is 1 for ALFOSC which has a Primary extension with no data
        and the Image extension containing the raw data.

    Returns
    =======

    norm_flat : np.array (N, M)
        Normalized 2D flat field image, same shape as in input image.

    """

    HDU = pf.open(fname)
    if len(HDU)+1 <= ext:
        flat = HDU[ext].data
    else:
        ext = 0
        flat = HDU[0].data

    if ext > 0 and HDU[0].size == 0:
        # No data in first extension, merge headers:
        hdr = HDU[0].header
        for key in HDU[1].header.keys():
            hdr[key] = HDU[1].header[key]

    else:
        hdr = HDU[ext].header

    if verbose:
        print ""
        print "Running task:  Normalization of Spectral Flat Field"
        print ""
        print "  Input file:"
        HDU.info()

    flat1D = np.mean(flat, axis)
    x = np.arange(len(flat1D))
    x2 = x2 / hdr['DETYBIN']
    fit = Chebyshev.fit(x[x1:x2], flat1D[x1:x2], order)

    flat_model = gaussian_filter1d(flat1D, sigma)

    # substitute the fit in the ends to remove convolution effects:
    dx = len(x)-x2
    ycut = len(x) - 2*dx
    flat_model[:3*sigma] = fit(x[:3*sigma])
    flat_model[ycut:] = fit(x[ycut:])

    # make 2D spectral shape:
    if axis == 1:
        model2D = np.resize(flat_model, flat.T.shape)
        model2D = model2D.T
    else:
        model2D = np.resize(flat_model, flat.shape)

    flat_norm = flat / model2D
    hdr['DATAMIN'] = np.min(flat_norm)
    hdr['DATAMAX'] = np.max(flat_norm)

    if plot:
        plt.close('all')
        fig2D = plt.figure()
        fig1D = plt.figure()

        ax1_2d = fig2D.add_subplot(121)
        ax2_2d = fig2D.add_subplot(122)
        ax1_2d.imshow(flat, origin='lower')
        ax1_2d.set_title("Raw Flat")
        std_norm = np.std(flat_norm[x1:x2, :])
        v1 = np.mean(flat_norm[x1:x2, :]) - 3*std_norm
        v2 = np.mean(flat_norm[x1:x2, :]) + 3*std_norm
        ax2_2d.imshow(flat_norm, origin='lower', vmin=v1, vmax=v2)
        ax2_2d.set_title("Normalized Flat")
        if axis == 1:
            ax1_2d.set_xlabel("Spatial Direction  [pixels along slit]")
            ax2_2d.set_xlabel("Spatial Direction  [pixels along slit]")
            ax1_2d.set_ylabel("Spectral Direction  [pixels along wavelength]")
        else:
            ax1_2d.set_ylabel("Spatial Direction  [pixels along slit]")
            ax1_2d.set_xlabel("Spectral Direction  [pixels along wavelength]")
            ax2_2d.set_xlabel("Spectral Direction  [pixels along wavelength]")

        ax1_1d = fig1D.add_subplot(211)
        ax2_1d = fig1D.add_subplot(212)

        residuals = flat1D - flat_model
        ax1_1d.plot(x, flat1D, 'k.')
        ax1_1d.plot(x, flat_model, 'crimson', lw=2, alpha=0.8)
        ax2_1d.plot(x, residuals, 'crimson', lw=2, alpha=0.8)
        ax2_1d.axhline(0., ls='--', color='k', lw=0.5)

        ax2_1d.set_xlabel("Spectral Direction  [pixels along wavelength]")

        power = np.floor(np.log10(np.max(flat1D))) - 1
        majFormatter = ticker.FuncFormatter(lambda x, p: my_formatter(x, p, power))
        ax1_1d.get_yaxis().set_major_formatter(majFormatter)
        ax1_1d.set_ylabel('Counts  [$10^{{{0:d}}}$ ADU]'.format(int(power)))

        power2 = np.floor(np.log10(np.max(residuals))) - 1
        majFormatter2 = ticker.FuncFormatter(lambda x, p: my_formatter(x, p, power2))
        ax2_1d.get_yaxis().set_major_formatter(majFormatter2)
        ax2_1d.set_ylabel('Residual  [$10^{{{0:d}}}$ ADU]'.format(int(power2)))
        noise = np.std(residuals[x1:x2])
        ax2_1d.set_ylim(-8*noise, 8*noise)

        ax1_1d.minorticks_on()
        ax2_1d.minorticks_on()

        if not exists("diagnostics"):
            os.mkdir("diagnostics")
        file_base = basename(fname)
        fname_root = file_base.strip('.fits')
        fig1D.savefig("diagnostics/specflat1d_"+fname_root+".pdf")
        fig2D.savefig("diagnostics/specflat2d_"+fname_root+".pdf")
        if show:
            plt.show()
        else:
            plt.close('all')

    if output:
        hdr['ORDER'] = (order, 'Order used for Chebyshev polynomial fit')
        hdr['NORMRMS'] = (noise, 'RMS noise of normalization [ADUs]')
        hdr.add_comment('Normalized Spectral Flat')
        hdr.add_comment('PyNOT version %r' % __version__)
        # save the file:
        if output[-5:] == '.fits':
            pass
        else:
            output += '.fits'
    else:
        grism = alfosc.grism_translate[hdr['ALGRNM']]
        output = 'NORM_FLAT_%s.fits' % grism

    if verbose:
        print ""
        print "  Output file saved:  %s" % output
    pf.writeto(output, flat_norm, header=hdr, clobber=clobber)

    return flat_norm


def fit_background(data, axis=1, x1=None, x2=None, interact=False, order=3, plot=True):
    """Fit background in 2D spectral data.
    The object trace should be masked out by defining x1 and x2 within
    which the background will not be fitted.
    The mask will automatically be defined if None is given (default).
    If interact is set to True, the mask will be interactively defined
    in a plotting window.
    The background is fitted by a Chebyshev polynomium (3rd order by default).

    Parameters
    ==========

    data : np.array (N, M)
        Spectral 2D data array

    axis : integer [default=1]
        Dispersion axis, 0: horizontal spectra, 1: vertical spectra

    x1 : integer [default=None]
        Mask pixels above this number in the fit to the background

    x2 : integer [default=None]
        Mask pixels below this number in the fit to the background

    interact : boolean [default=False]
        If True, this will open an interactive window to select the
        fitting regions. This is good if there are many objects on
        the slit, or if the trace is not well defined.

    order : integer [default=3]
        Order of the Chebyshev polynomium to fit the background

    Returns
    =======

    bg2D : np.array (N, M)
        Background model of the 2D frame, same shape as input data.
    """
    if axis == 0:
        # transpose the horizontal spectra to make them vertical:
        data = data.T

    x = np.arange(data.shape[1])
    # locate trace:
    if x1 is None or x2 is None:
        SPSF = np.median(data, 0)
        trace_center = np.argmax(SPSF)
        if interact:
            good = False
            plt.close('all')
            plt.figure()
            while good is False:
                plt.plot(x, SPSF, color='RoyalBlue')
                plt.title("Mark left and right bounds of fitting region")
                print "\nMark left and right bounds of fitting region"
                print ""
                sel = plt.ginput(-1, -1)
                mask = np.zeros(len(x), dtype=bool)
                sel = np.array(sel)
                if len(sel) % 2 == 0:
                    for x1, x2 in np.column_stack([sel[::2, 0], sel[1::2, 0]]):
                        mask += (x >= int(x1)) * (x <= int(x2))
                    masked_SPSF = np.ma.masked_where(~mask, SPSF)
                    plt.clf()
                    plt.plot(x, SPSF, color='0.6', lw=0.5)
                    plt.plot(x, masked_SPSF, color='k')
                    plt.draw()

                    answer = raw_input("Is the mask correct? [Y/n]  ")
                    if answer.lower() in ['n', 'no']:
                        good = False
                    else:
                        good = True

                else:
                    print "\n Something went wrong, the number of points must be even!\n"
                    continue

        else:
            width = 20
            x1 = trace_center - width
            x2 = trace_center + width
            obj = (x >= x1) * (x <= x2)
            mask = (x >= 20) * ~obj

    else:
        obj = (x >= x1) * (x <= x2)
        mask = (x >= 20) * ~obj

    bg2D = np.zeros_like(data)
    for i, row in enumerate(data):
        # Median filter the data to remove outliers:
        med_row = median_filter(row, 15)
        noise = mad(row)*1.4826
        this_mask = mask * (np.abs(row - med_row) < 10*noise)
        bg = Chebyshev.fit(x[this_mask], row[this_mask], order)
        bg2D[i] = bg(x)

    if axis == 0:
        # Rotate the data and the model back to horizontal orientation:
        data = data.T
        bg2D = bg2D.T

    plt.close('all')
    if plot:
        fig2D = plt.figure()
        ax1_2d = fig2D.add_subplot(121)
        ax2_2d = fig2D.add_subplot(122)
        noise = mad(data)
        v1 = np.median(data) - 3*noise
        v2 = np.median(data) + 3*noise
        ax1_2d.imshow(data, origin='lower', vmin=v1, vmax=v2)
        ax1_2d.set_title("Raw Data")
        ax2_2d.imshow(bg2D, origin='lower', vmin=v1, vmax=v2)
        ax2_2d.set_title("Background Model")
        if axis == 1:
            ax1_2d.set_xlabel("Spatial Direction  [pixels along slit]")
            ax2_2d.set_xlabel("Spatial Direction  [pixels along slit]")
            ax1_2d.set_ylabel("Spectral Direction  [pixels along wavelength]")
        else:
            ax1_2d.set_ylabel("Spatial Direction  [pixels along slit]")
            ax1_2d.set_xlabel("Spectral Direction  [pixels along wavelength]")
            ax2_2d.set_xlabel("Spectral Direction  [pixels along wavelength]")

        plt.show()

    return bg2D


def science_reduction_2d(sci_input, arc_frame, output='', bias='', flat='', trimx=[None, None],
                         trimy=[None, None], crr=True, axis=1, bg_x1=None, bg_x2=None,
                         bg_interact=False, bg_order=3, bg_plot=False, opt_ext=True,
                         loc_interact=True, ext_background=True, center_order=3, FWHM0=10,
                         loc_xmin=100, loc_xmax=-100, loc_binsize=20, aper_cen=None,
                         wl_order=4, sensitivity=None,
                         verbose=True, clobber=True, show=False):
    """
    Perform science reduction on a single input frame or a list of files.

    Parameters
    ==========

    sci_input : string or list
        Input science data to reduce.

    arc_frame : string or list
        Input arc spectrum or list of arc spectra for each science frame.

    output : string [default='']
        Output filename for final backgroung subtracted image.
        If not given, the output filename will be determined from
        OBJECT header keyword.

    bias : string
        Master bias to subtract from sci_input

    flat : string
        Normalized master flat

    trimx : list (2,)
        Lower and upper bounds, data outside the range will be trimmed

    trimy : list (2,)
        Lower and upper bounds, data outside the range will be trimmed

    crr : boolean [default=True]
        Perform cosmic ray rejection using IRAF.lacos_spec (van Dokkum 2001)

    axis : integer [default=1]
        Dispersion axis, 0: horizontal spectra, 1: vertical spectra

    bg : boolean [default=True]
        Subtract background model?

    bg_ : parameters passed to 'fit_backgroun()'

    verbose : boolean [default=True]
        If True, print status messages.

    clobber : boolean [default=True]
        Overwrite existing output file if True.

    opt_ext : boolean [default=True]
        Perform optimal extraction of 1D spectrum and automatically localize trace?

    loc_interact : boolean [default=True]
        Use interactive window to select trace center if automatic localization fails?
        If not, the aperture is centered on *aper_cen* which defaults to the center of the CCD.

    ext_background : boolean [default=True]
        Refine baclground subtraction by extracting the 1D background around the aperture?
        This is done by default in an aperture of width = FWHM0 on either side of the trace.

    center_order : integer [defualt=3]
        Polynomial order for the position of the trace along the CCD.
        The fitting is performed using a Chebyshev polynomium.

    FWHM0 : integer [default=10]
        Aperture width if optimal extraction is turned off or if localization fails.

    loc_xmin : integer [default=100]
        Lower boundary for trace fitting.

    loc_xmax : integer [default=-100]
        Upper boundary for trace fitting.

    loc_binsize : integer [default=20]
        The numer of spectral bins over which to average the trace before fitting.
    """
    if isfile(sci_input):
        sci_input = [sci_input]
    img0 = pf.getdata(sci_input[0])

    if hasattr(arc_frame, '__iter__'):
        arc_list = arc_frame
    else:
        arc_list = [arc_frame for dummy in sci_input]

    if isfile(bias):
        mbias = pf.getdata(bias)
    else:
        mbias = np.zeros_like(img0)
        print " WARNING - No master bias file provided!"

    if isfile(flat):
        mflat = pf.getdata(flat)
    else:
        mflat = np.ones_like(img0)
        print " WARNING - No master flat file provided!"

    for sci_fname, arc_frame in zip(sci_input, arc_list):
        HDU = pf.open(sci_fname)
        if len(HDU) > 1:
            if verbose:
                print " Merging extensions:"
                HDU.info()
                print ""
            hdr = HDU[0].header
            for key in HDU[1].header.keys():
                hdr[key] = HDU[1].header[key]
        else:
            hdr = pf.getheader(sci_fname)
        sci_raw = pf.getdata(sci_fname)

        x1, x2 = trimx
        y1, y2 = trimy
        sci = (sci_raw[y1:y2, x1:x2] - mbias[y1:y2, x1:x2])/mflat[y1:y2, x1:x2]

        # Calculate error image:
        if hdr['CCDNAME'] == 'CCD14':
            hdr['GAIN'] = 0.16
        g = hdr['GAIN']
        r = hdr['RDNOISE']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            err = np.sqrt(g*sci + r**2)/g

        # Subtract background:
        bg = fit_background(sci, axis=axis, x1=bg_x1, x2=bg_x2,
                            interact=bg_interact, plot=bg_plot,
                            order=bg_order)
        sci = sci - bg

        # Detect and correct cosmic ray hits:
        if crr and astroscrappy_installed:
            mask, sci = detect_cosmics(sci, gain=hdr['GAIN'], readnoise=hdr['RDNOISE'],
                                       niter=4, verbose=verbose)
            # Add comment to FITS header:
            hdr.add_comment("Cosmic Ray Rejection using Astroscrappy (based on van Dokkum 2001)")
            # expand mask to neighbouring pixels:
            big_mask = np.zeros_like(mask)
            for shift, axis in [(1, 0), (-1, 0), (1, 1), (-1, 1)]:
                big_mask += np.roll(mask, shift, axis)
            mask = 4 * (big_mask > 0)

        else:
            mask = np.zeros_like(sci)

        bg_err = np.sqrt(g*bg + r**2)/g
        # Fix NaN values from negative pixel values:
        err_NaN = np.isnan(err)
        err[err_NaN] = bg_err[err_NaN]

        sci_ext = pf.PrimaryHDU(sci, header=hdr)
        err_ext = pf.ImageHDU(err, header=hdr, name='ERR')
        mask_ext = pf.ImageHDU(mask, name='MASK')
        bg_ext = pf.ImageHDU(bg, name='SKY')

        output_HDU = pf.HDUList([sci_ext, err_ext, mask_ext, bg_ext])

        if len(output) > 0:
            if output[-5:] == '.fits':
                pass
            else:
                output += '.fits'
        else:
            output = "skysub2D_%s.fits" % hdr['OBJECT']

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            output_HDU.writeto(output, clobber=clobber)

        # Pass the corrected 2D spectrum to extraction and calibration:
        extract_and_calibrate(output, arc_frame, bin_size=loc_binsize, xmin=loc_xmin,
                              xmax=loc_xmax, do_opt_extract=opt_ext, interact=loc_interact,
                              background=ext_background, center_order=center_order,
                              FWHM0=FWHM0, trimx=trimx, trimy=trimy, wl_order=wl_order,
                              aper_cen=aper_cen, sensitivity=sensitivity, show=show)


if __name__ == '__main__':
    def str2bool(v):
        return v.lower() in ("yes", "true", "t", "1")

    plt.interactive(True)
    parser = ArgumentParser()
    parser.add_argument("sci_in", type=str,
                        help="Raw science spectrum or file containing a list")
    parser.add_argument("arc_in", type=str,
                        help="Raw arc spectrum or file containing a list")
    parser.add_argument("--bias", type=str, default='MASTER_BIAS.fits',
                        help="Master BIAS frame")
    parser.add_argument("--flat", type=str, default='NORM_FLAT.fits',
                        help="Normalized master flat frame")
    parser.add_argument("--no-crr", action="store_false",
                        help="When this option is set, no cosmic ray rejection is performed")
    parser.add_argument("-x", "--trimx", nargs=2, type=int, default=[None, None],
                        help="Give lower and upper limit for the x-range to trim")
    parser.add_argument("-y", "--trimy", nargs=2, type=int, default=[None, None],
                        help="Give lower and upper limit for the y-range to trim")
    parser.add_argument("--axis", type=int, default=1,
                        help="Dispersion axis, 0: horizontal, 1: vertical")
    parser.add_argument("--bg-x1", type=int, default=1,
                        help="Lower boundary of row pixels for fit to background")
    parser.add_argument("--bg-x2", type=int, default=1,
                        help="Upper boundary of row pixels for fit to background")
    parser.add_argument("--bg-interact", action="store_true",
                        help="Interactively select background region?")
    parser.add_argument("--bg-order", type=int, default=3,
                        help="Order for background polynomium")
    parser.add_argument("--bg-extract", type=str2bool, default=True,
                        help="Extract 1D background and refine subtraction?")
    parser.add_argument("--opt-ext", type=str2bool, default=True,
                        help="Disable optimal extraction")
    parser.add_argument("--loc-interact", action="store_true",
                        help="Interactively select aperture center")
    parser.add_argument("--loc-xmin", type=int, default=100,
                        help="Lower boundary on pixels for localization fitting")
    parser.add_argument("--loc-xmax", type=int, default=-100,
                        help="Upper boundary on pixels for localization fitting")
    parser.add_argument("--loc-binsize", type=int, default=30,
                        help="Number of spectral bins to combine for fitting of trace")
    parser.add_argument("--cen-order", type=int, default=3,
                        help="Order for trace centroid polynomium")
    parser.add_argument("--fwhm", type=int, default=10,
                        help="Default aperture width (+/- 1.5 x FWHM)")
    parser.add_argument("--aper-cen", type=int, default=None,
                        help="Center of aperture if automatic localization fails, default is center of CCD")
    parser.add_argument("--wl-order", type=int, default=4,
                        help="Order for wavelength solution polynomium")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="Print status updates")
    parser.add_argument("-s", "--show", action="store_true",
                        help="Show 1D extraction and diagnostics")

    args = parser.parse_args()

    if isfile(args.sci_in) and args.sci_in[-5:] == '.fits':
        sci_input = args.sci_in
    else:
        sci_input = np.loadtxt(args.sci_in, usecols=(0,), dtype=str)

    if isfile(args.arc_in) and args.arc_in[-5:] == '.fits':
        arc_frame = args.arc_in
    else:
        arc_frame = np.loadtxt(args.arc_in, usecols=(0,), dtype=str)

    science_reduction_2d(sci_input, arc_frame, bias=args.bias, flat=args.flat, trimx=args.trimx,
                         trimy=args.trimy, crr=args.no_crr, axis=args.axis, bg_x1=args.bg_x1,
                         bg_x2=args.bg_x2,
                         bg_interact=args.bg_interact, bg_order=args.bg_order,
                         opt_ext=args.opt_ext, loc_interact=args.loc_interact,
                         ext_background=args.bg_extract,
                         FWHM0=args.fwhm, aper_cen=args.aper_cen,
                         loc_xmin=args.loc_xmin, loc_xmax=args.loc_xmax,
                         loc_binsize=args.loc_binsize, center_order=args.cen_order,
                         wl_order=args.wl_order, verbose=args.verbose, show=args.show)
